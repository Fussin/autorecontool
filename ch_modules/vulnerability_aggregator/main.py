import json
import os
import logging
from datetime import datetime, timezone
from .parsers import PARSER_MAPPING # Import the mapping

# Configure logging for the aggregator
logger = logging.getLogger(__name__)
# Ensure a handler is added if not configured by root logger,
# especially if this module is run standalone or before global logging setup.
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)


def generate_deduplication_key(vuln_obj):
    """
    Generates a key for deduplication based on vulnerability type, target URL,
    and affected component/parameter.
    Normalizes URL by removing query string and fragment for broader matching if parameter is not specific.
    """
    key_parts = [
        vuln_obj.get("vulnerability_type", ""), # Use the full vulnerability type for more specific deduplication
        vuln_obj.get("target_url", "").split("?")[0].split("#")[0] # URL without query/fragment
    ]

    affected_component = vuln_obj.get("affected_component")
    if affected_component:
        # Normalize parameter names, e.g., "Parameter: query" -> "query"
        comp_name = affected_component.split(":")[-1].strip().lower()
        key_parts.append(comp_name)
    else:
        # For vulns not tied to a specific param (e.g. some CORS, Sensitive File Exposure)
        # The URL base and type should be enough.
        key_parts.append("N/A")

    return tuple(sorted(key_parts))


def merge_vulnerabilities(existing_vuln, new_vuln):
    """
    Merges a new vulnerability finding into an existing one.
    - Combines source_modules.
    - Prefers higher severity and confidence.
    - Appends evidence if different and informative.
    - Updates last_seen_timestamp.
    """
    # Combine source_modules without duplicates
    existing_vuln["source_modules"] = sorted(list(set(existing_vuln["source_modules"] + new_vuln["source_modules"])))

    # Update severity (take the higher one - simplistic approach for now)
    severity_order = {"Informational": 0, "Low": 1, "Medium": 2, "High": 3, "Critical": 4}
    if severity_order.get(new_vuln["severity"], -1) > severity_order.get(existing_vuln["severity"], -1):
        existing_vuln["severity"] = new_vuln["severity"]

    # Update confidence (take the higher one)
    confidence_order = {"Low": 0, "Medium": 1, "High": 2}
    if confidence_order.get(new_vuln["confidence"], -1) > confidence_order.get(existing_vuln["confidence"], -1):
        existing_vuln["confidence"] = new_vuln["confidence"]

    # Append evidence if new evidence is different and potentially more informative
    # This is a simple append; more sophisticated merging could be done
    if new_vuln.get("evidence") and new_vuln.get("evidence") not in existing_vuln.get("evidence", ""):
        existing_vuln["evidence"] = f"{existing_vuln.get('evidence', '')} | Additional evidence from {new_vuln['source_modules'][-1]}: {new_vuln['evidence']}"

    # Update payload if the new one is more specific or different
    if new_vuln.get("payload") and new_vuln.get("payload") != existing_vuln.get("payload"):
        if not existing_vuln.get("payload"):
            existing_vuln["payload"] = new_vuln.get("payload")
        else: # append if different, could be a list of payloads in future
             existing_vuln["payload"] += f" | {new_vuln.get('payload')}"


    # Update timestamps
    existing_vuln["last_seen_timestamp"] = max(existing_vuln["last_seen_timestamp"], new_vuln["last_seen_timestamp"])
    existing_vuln["first_seen_timestamp"] = min(existing_vuln["first_seen_timestamp"], new_vuln["first_seen_timestamp"])

    # Merge CWE IDs
    if new_vuln.get("cwe_ids"):
        existing_cwe = set(existing_vuln.get("cwe_ids", []))
        new_cwe = set(new_vuln.get("cwe_ids", []))
        existing_vuln["cwe_ids"] = sorted(list(existing_cwe.union(new_cwe)))

    # Tags
    if new_vuln.get("tags"):
        existing_tags = set(existing_vuln.get("tags", []))
        new_tags = set(new_vuln.get("tags", []))
        existing_vuln["tags"] = sorted(list(existing_tags.union(new_tags)))

    # Prefer more detailed description if one is significantly longer or more specific
    if len(new_vuln.get("description", "")) > len(existing_vuln.get("description", "")):
         if new_vuln.get("description") not in existing_vuln.get("description", "") : # Avoid simple repetition
            existing_vuln["description"] = f"{existing_vuln.get('description','')} | Updated Detail: {new_vuln.get('description','')}"


    return existing_vuln


def aggregate_and_deduplicate_vulnerabilities(scan_output_dir, scan_id):
    """
    Collects vulnerability data from all parser modules, normalizes it,
    deduplicates findings, and saves the aggregated results.

    Args:
        scan_output_dir (str): The directory where individual module outputs are stored.
        scan_id (str): The ID of the current scan job.

    Returns:
        str: Path to the aggregated vulnerabilities JSON file, or None if an error occurs.
    """
    logger.info(f"Starting vulnerability aggregation for scan ID: {scan_id} in dir: {scan_output_dir}")
    all_parsed_vulnerabilities = []

    for vuln_type, parser_info in PARSER_MAPPING.items():
        parser_func = parser_info["parser_func"]
        results_filename = parser_info["filename"]
        json_file_path = os.path.join(scan_output_dir, results_filename)

        if os.path.exists(json_file_path):
            logger.info(f"Parsing {results_filename} using {parser_func.__name__}...")
            try:
                parsed_vulns = parser_func(json_file_path, scan_id)
                if parsed_vulns:
                    all_parsed_vulnerabilities.extend(parsed_vulns)
                    logger.info(f"Successfully parsed {len(parsed_vulns)} vulnerabilities from {results_filename}.")
                else:
                    logger.info(f"No vulnerabilities found or parsed from {results_filename}.")
            except Exception as e:
                logger.error(f"Error parsing {results_filename} with {parser_func.__name__}: {e}", exc_info=True)
        else:
            logger.warning(f"Results file not found, skipping: {json_file_path}")

    if not all_parsed_vulnerabilities:
        logger.info("No vulnerabilities were parsed from any module outputs.")
        # Still create an empty aggregated file for consistency
        final_vulnerabilities_list = []
    else:
        logger.info(f"Total parsed vulnerabilities before deduplication: {len(all_parsed_vulnerabilities)}")

        # Deduplication logic
        deduplicated_map = {}
        for vuln_obj in all_parsed_vulnerabilities:
            key = generate_deduplication_key(vuln_obj)
            if key in deduplicated_map:
                logger.debug(f"Duplicate found for key {key}. Merging vulnerability: {vuln_obj['id']}")
                deduplicated_map[key] = merge_vulnerabilities(deduplicated_map[key], vuln_obj)
            else:
                deduplicated_map[key] = vuln_obj

        final_vulnerabilities_list = list(deduplicated_map.values())
        logger.info(f"Total vulnerabilities after deduplication: {len(final_vulnerabilities_list)}")

    # Output Generation
    aggregated_output_filename = "aggregated_vulnerabilities.json"
    aggregated_output_path = os.path.join(scan_output_dir, aggregated_output_filename)

    try:
        with open(aggregated_output_path, 'w') as f:
            json.dump(final_vulnerabilities_list, f, indent=2)
        logger.info(f"Aggregated vulnerabilities saved to: {aggregated_output_path}")
        return aggregated_output_path
    except IOError as e:
        logger.error(f"Failed to write aggregated vulnerabilities file: {e}")
        return None

if __name__ == '__main__':
    # Example usage for direct testing of the aggregator
    # This requires dummy output files from parsers to be present in a test directory.

    # Setup a dummy scan environment
    test_scan_id = "aggregator_test_scan_001"
    dummy_scan_dir = os.path.join("instance", "scan_outputs", "dummy_aggregator_target.com")
    os.makedirs(dummy_scan_dir, exist_ok=True)

    logger.info(f"Simulating aggregator run. Test scan directory: {dummy_scan_dir}")

    # Create dummy XSS output
    dummy_xss_data = {
        "vulnerabilities": [
            {"url": "http://dummy_aggregator_target.com/search?q=test1", "param": "q", "type": "Reflected XSS", "tool": "XSSScan1", "evidence": "Evidence A"},
            {"url": "http://dummy_aggregator_target.com/search?q=test1", "param": "q", "type": "Reflected XSS", "tool": "XSSScan2", "evidence": "Evidence B"}, # Duplicate
            {"url": "http://dummy_aggregator_target.com/profile#test2", "type": "DOM XSS", "tool": "DOMScan", "evidence": "DOM Evidence"}
        ]
    }
    with open(os.path.join(dummy_scan_dir, PARSER_MAPPING["xss"]["filename"]), 'w') as f:
        json.dump(dummy_xss_data, f)

    # Create dummy SQLi output
    dummy_sqli_data = {
        "vulnerabilities": [
            {"url": "http://dummy_aggregator_target.com/product?id=1", "parameter": "id", "technique": "Error-based", "dbms": "MySQL", "sqlmap_output_dir": "/tmp/sqlmap1"},
            {"url": "http://dummy_aggregator_target.com/product?id=1", "parameter": "id", "technique": "Time-based", "dbms": "MySQL", "tool": "SQLiTool2"} # Duplicate by key
        ]
    }
    with open(os.path.join(dummy_scan_dir, PARSER_MAPPING["sqli"]["filename"]), 'w') as f:
        json.dump(dummy_sqli_data, f)

    # Create dummy LFI (empty for this test to see handling)
    dummy_lfi_data = {"vulnerabilities": []}
    with open(os.path.join(dummy_scan_dir, PARSER_MAPPING["lfi"]["filename"]), 'w') as f:
        json.dump(dummy_lfi_data, f)

    # Run aggregator
    aggregated_file = aggregate_and_deduplicate_vulnerabilities(dummy_scan_dir, test_scan_id)

    if aggregated_file and os.path.exists(aggregated_file):
        logger.info(f"Aggregator test successful. Output: {aggregated_file}")
        with open(aggregated_file, 'r') as f:
            final_data = json.load(f)
            logger.info(f"Number of unique vulnerabilities in aggregated file: {len(final_data)}")
            # print(json.dumps(final_data, indent=2)) # Optionally print full output

            # Basic assertions for test
            # Expected: 1 XSS (merged), 1 DOM XSS, 1 SQLi (merged) = 3 total unique vulns
            assert len(final_data) == 3, f"Expected 3 unique vulnerabilities, got {len(final_data)}"

            # Check if XSS was merged
            xss_merged = False
            sqli_merged = False
            for v_obj in final_data:
                if "Reflected XSS" in v_obj["vulnerability_type"] and "dummy_aggregator_target.com/search" in v_obj["target_url"]:
                    assert len(v_obj["source_modules"]) == 2, "XSS sources not merged correctly"
                    assert "XSSScan1" in v_obj["source_modules"] and "XSSScan2" in v_obj["source_modules"]
                    assert "Evidence A" in v_obj["evidence"] and "Evidence B" in v_obj["evidence"]
                    xss_merged = True
                if "SQL Injection" in v_obj["vulnerability_type"] and "dummy_aggregator_target.com/product" in v_obj["target_url"]:
                    assert "SQLMap" in v_obj["source_modules"] and "SQLiTool2" in v_obj["source_modules"]
                    sqli_merged = True


            assert xss_merged, "Merged XSS vulnerability not found as expected."
            assert sqli_merged, "Merged SQLi vulnerability not found as expected."
            logger.info("Aggregator test assertions passed.")

    else:
        logger.error("Aggregator test failed.")

    # Clean up dummy dir (optional)
    # import shutil
    # shutil.rmtree(dummy_scan_dir)
    # logger.info(f"Cleaned up dummy scan directory: {dummy_scan_dir}")
